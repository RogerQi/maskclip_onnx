{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/anaconda3/envs/hdt/lib/python3.8/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/roger/anaconda3/envs/hdt/lib/python3.8/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import maskclip_onnx\n",
    "\n",
    "class MaskclipBackbone(nn.Module):\n",
    "    def __init__(self, model_name=\"ViT-B/16\", convert_to_fp16=False):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model, _ = maskclip_onnx.clip.load(\n",
    "            model_name,\n",
    "            download_root=os.getenv('TORCH_HOME', os.path.join(os.path.expanduser('~'), '.cache', 'torch')),\n",
    "            convert_to_fp16=convert_to_fp16  # has to be false for ONNX export in torch>=2.0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        features = self.model.get_patch_encodings(img)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clip_backbone = MaskclipBackbone(convert_to_fp16=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "torch.manual_seed(0)\n",
    "test_tensor = torch.randn((64, 3, 240, 320), dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maskclip inference time: 100.33645868301392 ms\n"
     ]
    }
   ],
   "source": [
    "# FP16 inference\n",
    "my_clip_backbone = MaskclipBackbone(convert_to_fp16=True).cuda().eval()\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        output = my_clip_backbone(test_tensor)\n",
    "\n",
    "    start_cp = time.time()\n",
    "    TEST_TIME = 100\n",
    "    for _ in range(TEST_TIME):\n",
    "        output = my_clip_backbone(test_tensor)\n",
    "\n",
    "    end_cp = time.time()\n",
    "print(f\"Maskclip inference time: {(end_cp - start_cp) / TEST_TIME * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maskclip inference time: 256.88424348831177 ms\n"
     ]
    }
   ],
   "source": [
    "# FP32 inference. Subsequent steps need to use FP32 version.\n",
    "my_clip_backbone = MaskclipBackbone(convert_to_fp16=False).cuda().eval()\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        output = my_clip_backbone(test_tensor)\n",
    "\n",
    "    start_cp = time.time()\n",
    "    TEST_TIME = 100\n",
    "    for _ in range(TEST_TIME):\n",
    "        output = my_clip_backbone(test_tensor)\n",
    "\n",
    "    end_cp = time.time()\n",
    "print(f\"Maskclip inference time: {(end_cp - start_cp) / TEST_TIME * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:20: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_patches == num_og_patches and w == h:\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:31: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert w0 * h0 == num_patches, \"Number of patches does not match\"\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:38: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  patch_per_ax = int(np.sqrt(num_og_patches))\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:42: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale_factor=(float(w0 / patch_per_ax), float(h0 / patch_per_ax)),\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:48: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  int(w0) == patch_pos_embed_interp.shape[-2] and int(h0) == patch_pos_embed_interp.shape[-1]\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "# onnx_program = torch.onnx.dynamo_export(my_export_wrapper, test_tensor)\n",
    "torch.onnx.export(\n",
    "    my_clip_backbone,                  # model to export\n",
    "    test_tensor,        # inputs of the model,\n",
    "    \"test_model.onnx\",        # filename of the ONNX model\n",
    "    input_names = ['input'],   # the model's input names\n",
    "    export_params=True,\n",
    ")\n",
    "\n",
    "# torch.onnx.export(my_export_model, input_tensor, 'exported_clip.onnx', export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskclip_onnx.onnx_tensorrt import TensorRTBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST FP16 detected. Enabling precision to FP16...\n",
      "Loading serialized engine from test_model_fp16.trt\n"
     ]
    }
   ],
   "source": [
    "trt_engine = TensorRTBackend.prepare(\"test_model.onnx\",\n",
    "                                        device='CUDA:0',\n",
    "                                        serialize_engine=True,\n",
    "                                        verbose=False,\n",
    "                                        serialized_engine_path=\"test_model.trt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_trt = trt_engine.run(test_tensor, 'torch_cuda')\n",
    "with torch.no_grad():\n",
    "    output_vanilla = my_clip_backbone(test_tensor).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_trt[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True, False]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True, False],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [False,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [False,  True,  True, ...,  True,  True,  True],\n",
       "        [ True, False,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ..., False,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(output_trt[0].cpu().numpy(), output_vanilla, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maskclip inference time: 90.04132509231567 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Profiling with device-host-device-host-device memory transfer\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        arr = test_tensor.cpu().numpy()\n",
    "        output = trt_engine.run(arr)\n",
    "        output = torch.tensor(output[0]).cuda()\n",
    "\n",
    "    start_cp = time.time()\n",
    "    TEST_TIME = 100\n",
    "    for _ in range(TEST_TIME):\n",
    "        arr = test_tensor.cpu().numpy()\n",
    "        output = trt_engine.run(arr)\n",
    "        output = torch.tensor(output[0]).cuda()\n",
    "\n",
    "    end_cp = time.time()\n",
    "print(f\"Maskclip inference time: {(end_cp - start_cp) / TEST_TIME * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maskclip inference time: 51.995224952697754 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Profiling with host-device-host memory transfer\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    arr = test_tensor.cpu().numpy()\n",
    "    for _ in range(10):\n",
    "        output = trt_engine.run(arr)\n",
    "\n",
    "    start_cp = time.time()\n",
    "    TEST_TIME = 100\n",
    "    for _ in range(TEST_TIME):\n",
    "        output = trt_engine.run(arr)\n",
    "\n",
    "    end_cp = time.time()\n",
    "print(f\"Maskclip inference time: {(end_cp - start_cp) / TEST_TIME * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maskclip inference time: 47.76280879974365 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Profiling with device-host memory transfer\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        output = trt_engine.run(test_tensor, input_output_mode='torch_cuda')\n",
    "\n",
    "    start_cp = time.time()\n",
    "    TEST_TIME = 100\n",
    "    for _ in range(TEST_TIME):\n",
    "        output = trt_engine.run(test_tensor, input_output_mode='torch_cuda')\n",
    "\n",
    "    end_cp = time.time()\n",
    "print(f\"Maskclip inference time: {(end_cp - start_cp) / TEST_TIME * 1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
