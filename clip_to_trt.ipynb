{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbc52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from einops import rearrange, einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458db0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/anaconda3/envs/hdt/lib/python3.8/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/roger/anaconda3/envs/hdt/lib/python3.8/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import maskclip_onnx\n",
    "\n",
    "def load_clip():\n",
    "    \"\"\"\n",
    "    Load CLIP and its preprocessing transforms.\n",
    "    Cache it, so we only have to do it once.\n",
    "    \"\"\"\n",
    "    model, preprocess = maskclip_onnx.clip.load(\n",
    "        name='ViT-L/14@336px', device=torch.device(\"cuda\")\n",
    "    )\n",
    "    return model, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56a4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = load_clip()\n",
    "\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d653a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class export_wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, images):\n",
    "        aspect_ratio = images.shape[-1] / images.shape[-2]\n",
    "        target_H = 336\n",
    "        target_W = int(target_H * aspect_ratio)\n",
    "        images = F.interpolate(images, size=(target_H, target_W), mode='bilinear', align_corners=False)\n",
    "        # normalize\n",
    "        rgb_mean = torch.tensor((0.48145466, 0.4578275, 0.40821073)).reshape(1, 3, 1, 1).to('cuda')\n",
    "        rgb_std  = torch.tensor((0.26862954, 0.26130258, 0.27577711)).reshape(1, 3, 1, 1).to('cuda')\n",
    "        images = (images - rgb_mean) / rgb_std\n",
    "\n",
    "        # Get CLIP embeddings for the images\n",
    "        # with logger.time(\"get_clip_embeddings\"):\n",
    "        patch_embeddings = model.get_patch_encodings(images)\n",
    "        # patch_embeddings = []\n",
    "        # patch_embeddings.append(model.get_patch_encodings(preprocessed_images))\n",
    "        # patch_embeddings = torch.cat(patch_embeddings, dim=0)\n",
    "\n",
    "        # Reshape embeddings to number of patches in height and width\n",
    "#         h_in, w_in = images.shape[-2:]\n",
    "#         h_out = h_in // model.visual.patch_size\n",
    "#         w_out = w_in // model.visual.patch_size\n",
    "#         embeddings = rearrange(patch_embeddings, \"b (h w) d -> b d h w\", h=h_out, w=w_out)\n",
    "        return patch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ea45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_export_model = export_wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9a4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors = torch.randn(1, 3, 240, 320).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0f3749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 240, 320])\n",
      "tensor(4.6410, device='cuda:0')\n",
      "tensor(-5.2805, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_tensor = image_tensors[0].unsqueeze(0)\n",
    "\n",
    "input_tensor = input_tensor.float().cuda()\n",
    "print(input_tensor.shape)\n",
    "print(input_tensor.max())\n",
    "print(input_tensor.min())\n",
    "my_export_model = my_export_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f2b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14924/236815134.py:11: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  target_W = int(target_H * aspect_ratio)\n",
      "/tmp/ipykernel_14924/236815134.py:14: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  rgb_mean = torch.tensor((0.48145466, 0.4578275, 0.40821073)).reshape(1, 3, 1, 1).to('cuda')\n",
      "/tmp/ipykernel_14924/236815134.py:15: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  rgb_std  = torch.tensor((0.26862954, 0.26130258, 0.27577711)).reshape(1, 3, 1, 1).to('cuda')\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_patches == num_og_patches and w == h:\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert w0 * h0 == num_patches, \"Number of patches does not match\"\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:37: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  patch_per_ax = int(np.sqrt(num_og_patches))\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:41: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale_factor=(float(w0 / patch_per_ax), float(h0 / patch_per_ax)),\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:47: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  int(w0) == patch_pos_embed_interp.shape[-2] and int(h0) == patch_pos_embed_interp.shape[-1]\n",
      "/home/roger/maskclip_onnx/maskclip_onnx/interpolate.py:46: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n",
      "/home/roger/anaconda3/envs/hdt/lib/python3.8/site-packages/torch/onnx/utils.py:739: UserWarning: Constant folding in symbolic shape inference fails: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:445.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_export_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexported_clip.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hdt/lib/python3.8/site-packages/torch/onnx/utils.py:551\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport destination must be specified for torchscript-onnx export.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m     )\n\u001b[0;32m--> 551\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hdt/lib/python3.8/site-packages/torch/onnx/utils.py:1648\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1646\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1648\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1663\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1664\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/hdt/lib/python3.8/site-packages/torch/onnx/utils.py:1237\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m training \u001b[38;5;241m==\u001b[39m _C_onnx\u001b[38;5;241m.\u001b[39mTrainingMode\u001b[38;5;241m.\u001b[39mEVAL:\n\u001b[1;32m   1235\u001b[0m         params_dict \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_eval_peephole(graph, params_dict)\n\u001b[0;32m-> 1237\u001b[0m     params_dict \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx_constant_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLOBALS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx_opset_version\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GLOBALS\u001b[38;5;241m.\u001b[39monnx_shape_inference:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(my_export_model, input_tensor, 'exported_clip.onnx', export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c11ce536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1008, 768])\n",
      "tensor(14.6705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(-13.8217, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "raw_descriptor = my_export_model(input_tensor)\n",
    "print(raw_descriptor.shape)\n",
    "print(raw_descriptor.max())\n",
    "print(raw_descriptor.min())\n",
    "print(raw_descriptor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d49f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 64.9676730632782\n",
      "FPS: 15.392270537779687\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_cp = time.time()\n",
    "\n",
    "for _ in range(1000):\n",
    "    ret = my_export_model(input_tensor)\n",
    "\n",
    "end_cp = time.time()\n",
    "\n",
    "print(\"Total time: {}\".format(end_cp - start_cp))\n",
    "print(\"FPS: {}\".format(1000 / (end_cp - start_cp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23760f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
